{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhananjay-97/notebooks/blob/main/Chunking_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"\n",
        "Introduction\n",
        "\n",
        "Data Science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It draws from statistics, computer science, machine learning, and various data analysis techniques to discover patterns, make predictions, and derive actionable insights.\n",
        "\n",
        "Data Science can be applied across many industries, including healthcare, finance, marketing, and education, where it helps organizations make data-driven decisions, optimize processes, and understand customer behaviors.\n",
        "\n",
        "Overview of Big Data\n",
        "\n",
        "Big data refers to large, diverse sets of information that grow at ever-increasing rates. It encompasses the volume of information, the velocity or speed at which it is created and collected, and the variety or scope of the data points being covered.\n",
        "\n",
        "Data Science Methods\n",
        "\n",
        "There are several important methods used in Data Science:\n",
        "\n",
        "1. Regression Analysis\n",
        "2. Classification\n",
        "3. Clustering\n",
        "4. Neural Networks\n",
        "\n",
        "Challenges in Data Science\n",
        "\n",
        "- Data Quality: Poor data quality can lead to incorrect conclusions.\n",
        "- Data Privacy: Ensuring the privacy of sensitive information.\n",
        "- Scalability: Handling massive datasets efficiently.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Data Science continues to be a driving force in many industries, offering insights that can lead to better decisions and optimized outcomes. It remains an evolving field that incorporates the latest technological advancements.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "ok8dOLqairD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONS-T3N5iBVA"
      },
      "outputs": [],
      "source": [
        "def fixed_size_chunk(text, max_words=100):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
        "\n",
        "# Applying Fixed-Size Chunking\n",
        "fixed_chunks = fixed_size_chunk(sample_text)\n",
        "for chunk in fixed_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def sentence_chunk(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text for sent in doc.sents]\n",
        "\n",
        "# Applying Sentence-Based Chunking\n",
        "sentence_chunks = sentence_chunk(sample_text)\n",
        "for chunk in sentence_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ],
      "metadata": {
        "id": "tQFZVEGbiC19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def paragraph_chunk(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    return paragraphs\n",
        "\n",
        "# Applying Paragraph-Based Chunking\n",
        "paragraph_chunks = paragraph_chunk(sample_text)\n",
        "for chunk in paragraph_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ],
      "metadata": {
        "id": "PyhpGhFfiCz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_chunk(text, max_len=200):\n",
        "    doc = nlp(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    for sent in doc.sents:\n",
        "        current_chunk.append(sent.text)\n",
        "        if len(' '.join(current_chunk)) > max_len:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "# Applying Semantic-Based Chunking\n",
        "semantic_chunks = semantic_chunk(sample_text)\n",
        "for chunk in semantic_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ],
      "metadata": {
        "id": "pwHPh7J1iCxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modality_chunk(text, images=None, tables=None):\n",
        "    # This function assumes you have pre-processed text, images, and tables\n",
        "    text_chunks = paragraph_chunk(text)\n",
        "    return {'text_chunks': text_chunks, 'images': images, 'tables': tables}\n",
        "\n",
        "# Applying Modality-Specific Chunking\n",
        "modality_chunks = modality_chunk(sample_text, images=['img1.png'], tables=['table1'])\n",
        "print(modality_chunks)\n"
      ],
      "metadata": {
        "id": "2gGzIi49iCvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_window_chunk(text, chunk_size=100, overlap=20):\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk = ' '.join(tokens[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Applying Sliding Window Chunking\n",
        "sliding_chunks = sliding_window_chunk(sample_text)\n",
        "for chunk in sliding_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ],
      "metadata": {
        "id": "v4ifCoHziCs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hierarchical_chunk(text, section_keywords):\n",
        "    sections = []\n",
        "    current_section = []\n",
        "    for line in text.splitlines():\n",
        "        if any(keyword in line for keyword in section_keywords):\n",
        "            if current_section:\n",
        "                sections.append(\"\\n\".join(current_section))\n",
        "            current_section = [line]\n",
        "        else:\n",
        "            current_section.append(line)\n",
        "    if current_section:\n",
        "        sections.append(\"\\n\".join(current_section))\n",
        "    return sections\n",
        "\n",
        "# Applying Hierarchical Chunking\n",
        "section_keywords = [\"Introduction\", \"Overview\", \"Methods\", \"Conclusion\"]\n",
        "hierarchical_chunks = hierarchical_chunk(sample_text, section_keywords)\n",
        "for chunk in hierarchical_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ],
      "metadata": {
        "id": "QZEGgj7FiCqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def content_aware_chunk(text):\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    for line in text.splitlines():\n",
        "        if line.startswith(('##', '###', 'Introduction', 'Conclusion')):\n",
        "            if current_chunk:\n",
        "                chunks.append('\\n'.join(current_chunk))\n",
        "            current_chunk = [line]\n",
        "        else:\n",
        "            current_chunk.append(line)\n",
        "    if current_chunk:\n",
        "        chunks.append('\\n'.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "# Applying Content-Aware Chunking\n",
        "content_chunks = content_aware_chunk(sample_text)\n",
        "for chunk in content_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ],
      "metadata": {
        "id": "iYsYAHbyiCnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def table_aware_chunk(table):\n",
        "    return table.to_markdown()\n",
        "\n",
        "# Sample table data\n",
        "table = pd.DataFrame({\n",
        "    \"Name\": [\"John\", \"Alice\", \"Bob\"],\n",
        "    \"Age\": [25, 30, 22],\n",
        "    \"Occupation\": [\"Engineer\", \"Doctor\", \"Artist\"]\n",
        "})\n",
        "\n",
        "# Applying Table-Aware Chunking\n",
        "table_markdown = table_aware_chunk(table)\n",
        "print(table_markdown)\n"
      ],
      "metadata": {
        "id": "MKTyVEWkiCk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def token_based_chunk(text, max_tokens=200):\n",
        "    tokens = tokenizer(text)[\"input_ids\"]\n",
        "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
        "    return [tokenizer.decode(chunk) for chunk in chunks]\n",
        "\n",
        "# Applying Token-Based Chunking\n",
        "token_chunks = token_based_chunk(sample_text)\n",
        "for chunk in token_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ],
      "metadata": {
        "id": "QLzmiO3liCiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entity_based_chunk(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Applying Entity-Based Chunking\n",
        "entity_chunks = entity_based_chunk(sample_text)\n",
        "print(entity_chunks)\n"
      ],
      "metadata": {
        "id": "2os6JCNXiCfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def topic_based_chunk(text, num_topics=2):\n",
        "    vectorizer = CountVectorizer()\n",
        "    text_matrix = vectorizer.fit_transform([text])\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics)\n",
        "    lda.fit(text_matrix)\n",
        "    topics = lda.transform(text_matrix)\n",
        "    return topics\n",
        "\n",
        "# Applying Topic-Based Chunking\n",
        "topic_chunks = topic_based_chunk(sample_text)\n",
        "print(topic_chunks)\n"
      ],
      "metadata": {
        "id": "0aD9Ie9qiPDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "\n",
        "def topic_based_chunk(text, num_topics=3):\n",
        "    # Split the text into sentences for chunking\n",
        "    sentences = text.split('. ')\n",
        "\n",
        "    # Vectorize the sentences\n",
        "    vectorizer = CountVectorizer()\n",
        "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Apply LDA for topic modeling\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    lda.fit(sentence_vectors)\n",
        "\n",
        "    # Get the topic-word distribution\n",
        "    topic_word = lda.components_\n",
        "    vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Identify the top words for each topic\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(topic_word):\n",
        "        top_words_idx = topic.argsort()[:-6:-1]\n",
        "        topic_keywords = [vocabulary[i] for i in top_words_idx]\n",
        "        topics.append(\"Topic {}: {}\".format(topic_idx + 1, ', '.join(topic_keywords)))\n",
        "\n",
        "    # Generate chunks with topics\n",
        "    chunks_with_topics = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        topic_assignments = lda.transform(vectorizer.transform([sentence]))\n",
        "        assigned_topic = np.argmax(topic_assignments)\n",
        "        chunks_with_topics.append((topics[assigned_topic], sentence))\n",
        "\n",
        "    return chunks_with_topics\n",
        "\n",
        "\n",
        "# Get topic-based chunks\n",
        "topic_chunks = topic_based_chunk(sample_text, num_topics=3)\n",
        "\n",
        "# Display results\n",
        "for topic, chunk in topic_chunks:\n",
        "    print(f\"{topic}: {chunk}\\n\")"
      ],
      "metadata": {
        "id": "NqeHT9CoI78F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def page_based_chunk(pages):\n",
        "    # Split based on pre-processed page list (simulating PDF page text)\n",
        "    return pages\n",
        "\n",
        "# Sample pages\n",
        "pages = [\"Page 1 content\", \"Page 2 content\", \"Page 3 content\"]\n",
        "\n",
        "# Applying Page-Based Chunking\n",
        "page_chunks = page_based_chunk(pages)\n",
        "for chunk in page_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ],
      "metadata": {
        "id": "N6xijsBGiPAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_based_chunk(text, keywords):\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    for line in text.splitlines():\n",
        "        if any(keyword in line for keyword in keywords):\n",
        "            if current_chunk:\n",
        "                chunks.append('\\n'.join(current_chunk))\n",
        "            current_chunk = [line]\n",
        "        else:\n",
        "            current_chunk.append(line)\n",
        "    if current_chunk:\n",
        "        chunks.append('\\n'.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "# Applying Keyword-Based Chunking\n",
        "keywords = [\"Introduction\", \"Conclusion\", \"Methods\"]\n",
        "keyword_chunks = keyword_based_chunk(sample_text, keywords)\n",
        "for chunk in keyword_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ],
      "metadata": {
        "id": "UB1kMCzgiO9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_chunk(text):\n",
        "    paragraphs = paragraph_chunk(text)\n",
        "    hybrid_chunks = []\n",
        "    for paragraph in paragraphs:\n",
        "        hybrid_chunks += sentence_chunk(paragraph)\n",
        "    return hybrid_chunks\n",
        "\n",
        "# Applying Hybrid Chunking\n",
        "hybrid_chunks = hybrid_chunk(sample_text)\n",
        "for chunk in hybrid_chunks:\n",
        "    print(chunk, '\\n---\\n')\n"
      ],
      "metadata": {
        "id": "U7hmvkg6iO7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_keywords = [\"Introduction\", \"Overview\", \"Conclusion\"]\n",
        "\n",
        "# Define keywords for keyword-based chunking\n",
        "keywords = [\"Introduction\", \"Overview\", \"Conclusion\", \"Methods\", \"Challenges\"]\n",
        "\n",
        "# Updated wrapper function to call and display chunking strategies\n",
        "def apply_chunking_methods(text, strategies, section_keywords=None, keywords=None):\n",
        "    for strategy in strategies:\n",
        "        print(f\"\\n--- Applying {strategy.__name__} ---\\n\")\n",
        "\n",
        "        # Check if the strategy requires additional arguments\n",
        "        if strategy == hierarchical_chunk and section_keywords is not None:\n",
        "            chunks = strategy(text, section_keywords)\n",
        "        elif strategy == keyword_based_chunk and keywords is not None:\n",
        "            chunks = strategy(text, keywords)\n",
        "        else:\n",
        "            chunks = strategy(text)\n",
        "\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            print(f\"Chunk {idx+1}:\\n{chunk}\\n{'-'*50}\")\n",
        "\n",
        "# List of chunking strategies (functions) to apply\n",
        "strategies = [\n",
        "    fixed_size_chunk,\n",
        "    sentence_chunk,\n",
        "    paragraph_chunk,\n",
        "    semantic_chunk,\n",
        "    sliding_window_chunk,\n",
        "    hierarchical_chunk,  # Now properly handled\n",
        "    token_based_chunk,\n",
        "    entity_based_chunk,\n",
        "    topic_based_chunk,\n",
        "    page_based_chunk,\n",
        "    keyword_based_chunk,  # Now properly handled\n",
        "    hybrid_chunk,\n",
        "]\n"
      ],
      "metadata": {
        "id": "FX8ltVgjilAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apply_chunking_methods(sample_text, strategies, section_keywords, keywords)"
      ],
      "metadata": {
        "id": "FDOUsQjkiXeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EM8ldZmZi3Qr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}